{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ad1637",
   "metadata": {},
   "source": [
    "\n",
    "# MLP Training Notebook (PyTorch) — Spray Cone Angle / Penetration / Area\n",
    "\n",
    "**What it does**\n",
    "- Scans a root data directory containing subfolders `T1 ... T12`, each with CSV files `*_features.csv` (columns: `video, segment, frame, cone_angle, penetration_index, area`).\n",
    "- Infers experimental conditions (chamber pressure, injection pressure, injection duration, control back pressure) from T group and cine number.\n",
    "- Builds inputs [chamber_pressure, injection_pressure, injection_duration, control_backpressure] and targets [cone_angle, penetration_index, area].\n",
    "- Drops rows with undefined penetration (NaN) or saturated penetration (≥ 365 pixels).\n",
    "- Random train/val/test split with an option to re-generate it (toggle truth value).\n",
    "- Strong regularization: standardization, dropout, weight decay (L2), optional L1, gradient clipping, noise injection, batch/layer normalization, early-stopping, LR scheduling, mixed precision.\n",
    "- Saves: best model, scalers, split indices, training curves, predictions, metrics.\n",
    "\n",
    "> Data expectation: place your CSV files under a root directory like `C:/Users/Jiang/Documents/Mie_Py/Mie_Postprocessing_Py/Cine/`, within `T1`…`T12`. Each CSV should include the columns seen in your example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ef9c1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Imports\n",
    "import os, re, json, math, random, time, pathlib, copy, warnings\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# =====================\n",
    "# Configuration\n",
    "# =====================\n",
    "CONFIG = {\n",
    "    # Data\n",
    "    \"root_dir\": r\"C:/Users/Jiang/Documents/Mie_Py/Mie_Postprocessing_Py/Cine\",\n",
    "    \"csv_glob\": \"**/*_features.csv\",\n",
    "    \"saturation_px\": 365.0,\n",
    "    \"drop_nan_penetration\": True,\n",
    "    \"use_cuda_if_available\": True,\n",
    "    \"train_val_test_split\": [0.7, 0.15, 0.15],\n",
    "    \"seed\": 42,\n",
    "    \"REGENERATE_SPLIT\": True,\n",
    "    \"split_save_path\": \"splits_indices.json\",\n",
    "\n",
    "    # Features/Targets\n",
    "    \"feature_names\": [\"chamber_pressure\", \"injection_pressure\", \"injection_duration\", \"control_backpressure\"],\n",
    "    \"target_names\": [\"cone_angle\", \"penetration_index\", \"area\"],\n",
    "    \"standardize_features\": True,\n",
    "    \"standardize_targets\": True,\n",
    "\n",
    "    # Dataloader\n",
    "    \"batch_size\": 256,\n",
    "    \"num_workers\": 0,\n",
    "\n",
    "    # Model\n",
    "    \"hidden_sizes\": [128, 128, 64],\n",
    "    \"activation\": \"relu\",         # relu | gelu | prelu | tanh\n",
    "    \"dropout\": 0.2,\n",
    "    \"normalization\": \"batch\",     # none | batch | layer\n",
    "    \"weight_norm\": False,\n",
    "    \"output_activation\": \"none\",  # none | relu | softplus\n",
    "\n",
    "    # Optimization\n",
    "    \"epochs\": 200,\n",
    "    \"optimizer\": \"adamw\",\n",
    "    \"lr\": 3e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"l1_lambda\": 0.0,\n",
    "    \"gradient_clip_norm\": 1.0,\n",
    "\n",
    "    # Regularization tricks\n",
    "    \"input_noise_std\": 0.01,\n",
    "    \"mixup_alpha\": 0.0,           # 0 disables\n",
    "\n",
    "    # Scheduler\n",
    "    \"scheduler\": \"plateau\",       # none | plateau | cosine\n",
    "    \"plateau_patience\": 10,\n",
    "    \"plateau_factor\": 0.5,\n",
    "    \"cosine_T_max\": 100,\n",
    "\n",
    "    # Early stopping\n",
    "    \"early_stop_patience\": 20,\n",
    "    \"min_delta\": 1e-5,\n",
    "\n",
    "    # AMP\n",
    "    \"use_amp\": True,\n",
    "\n",
    "    # Outputs\n",
    "    \"out_dir\": \"runs_mlp\",\n",
    "}\n",
    "os.makedirs(CONFIG[\"out_dir\"], exist_ok=True)\n",
    "set_seed(CONFIG[\"seed\"])\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and CONFIG[\"use_cuda_if_available\"]) else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# =====================\n",
    "# Experimental mapping\n",
    "# =====================\n",
    "T_GROUP_TO_COND = {\n",
    "    1:  {\"chamber_pressure\": 5,  \"injection_pressure\": 2200, \"control_backpressure\": 1},\n",
    "    2:  {\"chamber_pressure\": 15, \"injection_pressure\": 2200, \"control_backpressure\": 1},\n",
    "    3:  {\"chamber_pressure\": 25, \"injection_pressure\": 2200, \"control_backpressure\": 1},\n",
    "    4:  {\"chamber_pressure\": 35, \"injection_pressure\": 2200, \"control_backpressure\": 1},\n",
    "    5:  {\"chamber_pressure\": 5,  \"injection_pressure\": 1400, \"control_backpressure\": 1},\n",
    "    6:  {\"chamber_pressure\": 15, \"injection_pressure\": 1400, \"control_backpressure\": 1},\n",
    "    7:  {\"chamber_pressure\": 35, \"injection_pressure\": 1400, \"control_backpressure\": 1},\n",
    "    8:  {\"chamber_pressure\": 5,  \"injection_pressure\": 2200, \"control_backpressure\": 4},\n",
    "    9:  {\"chamber_pressure\": 15, \"injection_pressure\": 2200, \"control_backpressure\": 4},\n",
    "    10: {\"chamber_pressure\": 35, \"injection_pressure\": 2200, \"control_backpressure\": 4},\n",
    "    11: {\"chamber_pressure\": 5,  \"injection_pressure\": 1600, \"control_backpressure\": 1},\n",
    "    12: {\"chamber_pressure\": 35, \"injection_pressure\": 1600, \"control_backpressure\": 1},\n",
    "}\n",
    "\n",
    "def cine_to_injection_duration_us(cine_number: int) -> float:\n",
    "    # 1..5 -> 340; +20 each block up to 91..95 -> 700\n",
    "    # 96..100 -> 750; then +50 per block up to 141..145 -> 1200\n",
    "    cine_number = max(1, min(145, int(cine_number)))\n",
    "    block = (cine_number - 1) // 5\n",
    "    if block <= 18:\n",
    "        return 340 + 20 * block\n",
    "    else:\n",
    "        return 750 + 50 * (block - 19)\n",
    "\n",
    "# =====================\n",
    "# Data ingestion\n",
    "# =====================\n",
    "def infer_t_group_from_path(path: str) -> Optional[int]:\n",
    "    m = re.search(r\"[\\\\/]T(\\d+)[\\\\/]\", path)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    m = re.search(r\"[\\\\/]T(\\d+)_\", path)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    return None\n",
    "\n",
    "def infer_cine_from_row_or_filename(row: pd.Series, csv_path: str) -> Optional[int]:\n",
    "    if \"video\" in row and pd.notna(row[\"video\"]):\n",
    "        try:\n",
    "            return int(row[\"video\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "    fname = os.path.basename(csv_path)\n",
    "    m = re.search(r\"(\\d+)\", fname)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    return None\n",
    "\n",
    "def scan_csvs(root_dir: str, pattern: str) -> List[str]:\n",
    "    files = [str(p) for p in pathlib.Path(root_dir).glob(pattern) if p.is_file()]\n",
    "    return files\n",
    "\n",
    "def build_dataframe(root_dir: str, pattern: str) -> pd.DataFrame:\n",
    "    csv_files = scan_csvs(root_dir, pattern)\n",
    "    rows = []\n",
    "    for csv_path in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Failed to read {csv_path}: {e}\")\n",
    "            continue\n",
    "        t_group = infer_t_group_from_path(csv_path)\n",
    "        if t_group is None:\n",
    "            warnings.warn(f\"Could not infer T group from path: {csv_path}. Skipping.\")\n",
    "            continue\n",
    "        needed = [\"video\", \"segment\", \"frame\", \"cone_angle\", \"penetration_index\", \"area\"]\n",
    "        miss = [c for c in needed if c not in df.columns]\n",
    "        if miss:\n",
    "            warnings.warn(f\"{csv_path} missing {miss}. Skipping file.\")\n",
    "            continue\n",
    "        df[\"_csv_path\"] = csv_path\n",
    "        df[\"_t_group\"] = t_group\n",
    "        if \"video\" in df.columns and pd.api.types.is_numeric_dtype(df[\"video\"]):\n",
    "            df[\"_cine\"] = df[\"video\"].astype(int)\n",
    "        else:\n",
    "            df[\"_cine\"] = df.apply(lambda r: infer_cine_from_row_or_filename(r, csv_path), axis=1)\n",
    "        rows.append(df)\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"video\",\"segment\",\"frame\",\"cone_angle\",\"penetration_index\",\"area\",\"_csv_path\",\"_t_group\",\"_cine\"])\n",
    "    big = pd.concat(rows, ignore_index=True)\n",
    "    return big\n",
    "\n",
    "def add_conditions_and_filter(df: pd.DataFrame, cfg: dict) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    cond_cols = []\n",
    "    for _, row in df.iterrows():\n",
    "        tg = int(row[\"_t_group\"])\n",
    "        cine = int(row[\"_cine\"]) if not pd.isna(row[\"_cine\"]) else None\n",
    "        if tg not in T_GROUP_TO_COND or cine is None:\n",
    "            cond_cols.append((np.nan, np.nan, np.nan, np.nan))\n",
    "            continue\n",
    "        cond = T_GROUP_TO_COND[tg]\n",
    "        inj_dur = cine_to_injection_duration_us(cine)\n",
    "        cond_cols.append((cond[\"chamber_pressure\"], cond[\"injection_pressure\"], inj_dur, cond[\"control_backpressure\"]))\n",
    "    cond_arr = np.array(cond_cols, dtype=float)\n",
    "    df[\"chamber_pressure\"] = cond_arr[:,0]\n",
    "    df[\"injection_pressure\"] = cond_arr[:,1]\n",
    "    df[\"injection_duration\"] = cond_arr[:,2]\n",
    "    df[\"control_backpressure\"] = cond_arr[:,3]\n",
    "\n",
    "    if cfg[\"drop_nan_penetration\"]:\n",
    "        df = df[~df[\"penetration_index\"].isna()]\n",
    "    if cfg[\"saturation_px\"] is not None:\n",
    "        df = df[df[\"penetration_index\"] < float(cfg[\"saturation_px\"])]\n",
    "    df = df.dropna(subset=cfg[\"feature_names\"] + cfg[\"target_names\"])\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# =====================\n",
    "# Split & scalers\n",
    "# =====================\n",
    "def save_split(path: str, train_idx, val_idx, test_idx):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump({\"train\": train_idx, \"val\": val_idx, \"test\": test_idx}, f)\n",
    "\n",
    "def load_split(path: str):\n",
    "    with open(path, \"r\") as f:\n",
    "        obj = json.load(f)\n",
    "    return obj[\"train\"], obj[\"val\"], obj[\"test\"]\n",
    "\n",
    "def make_split(n: int, ratios: List[float], seed: int) -> Tuple[List[int], List[int], List[int]]:\n",
    "    assert abs(sum(ratios) - 1.0) < 1e-6\n",
    "    idx = np.arange(n)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(idx)\n",
    "    n_train = int(n * ratios[0])\n",
    "    n_val = int(n * ratios[1])\n",
    "    train_idx = idx[:n_train].tolist()\n",
    "    val_idx = idx[n_train:n_train+n_val].tolist()\n",
    "    test_idx = idx[n_train+n_val:].tolist()\n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "class Standardizer:\n",
    "    def __init__(self):\n",
    "        self.mu = None\n",
    "        self.sigma = None\n",
    "    def fit(self, x: np.ndarray):\n",
    "        self.mu = np.nanmean(x, axis=0, keepdims=True)\n",
    "        self.sigma = np.nanstd(x, axis=0, keepdims=True) + 1e-8\n",
    "        return self\n",
    "    def transform(self, x: np.ndarray):\n",
    "        return (x - self.mu) / self.sigma\n",
    "    def inverse_transform(self, x: np.ndarray):\n",
    "        return x * self.sigma + self.mu\n",
    "    def state_dict(self):\n",
    "        return {\"mu\": self.mu.tolist(), \"sigma\": self.sigma.tolist()}\n",
    "    def load_state_dict(self, d):\n",
    "        self.mu = np.array(d[\"mu\"])\n",
    "        self.sigma = np.array(d[\"sigma\"])\n",
    "\n",
    "def build_tensors_and_split(df: pd.DataFrame, cfg: dict):\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\"No data available after filtering.\")\n",
    "    X = df[cfg[\"feature_names\"]].to_numpy(dtype=np.float32)\n",
    "    Y = df[cfg[\"target_names\"]].to_numpy(dtype=np.float32)\n",
    "\n",
    "    split_path = os.path.join(cfg[\"out_dir\"], cfg[\"split_save_path\"])\n",
    "    if cfg[\"REGENERATE_SPLIT\"] or (not os.path.exists(split_path)):\n",
    "        train_idx, val_idx, test_idx = make_split(len(df), cfg[\"train_val_test_split\"], cfg[\"seed\"])\n",
    "        save_split(split_path, train_idx, val_idx, test_idx)\n",
    "    else:\n",
    "        train_idx, val_idx, test_idx = load_split(split_path)\n",
    "\n",
    "    X_train, Y_train = X[train_idx], Y[train_idx]\n",
    "    X_val, Y_val = X[val_idx], Y[val_idx]\n",
    "    X_test, Y_test = X[test_idx], Y[test_idx]\n",
    "\n",
    "    x_scaler = Standardizer()\n",
    "    y_scaler = Standardizer()\n",
    "    if cfg[\"standardize_features\"]:\n",
    "        x_scaler.fit(X_train)\n",
    "        X_train = x_scaler.transform(X_train)\n",
    "        X_val = x_scaler.transform(X_val)\n",
    "        X_test = x_scaler.transform(X_test)\n",
    "    if cfg[\"standardize_targets\"]:\n",
    "        y_scaler.fit(Y_train)\n",
    "        Y_train = y_scaler.transform(Y_train)\n",
    "        Y_val = y_scaler.transform(Y_val)\n",
    "        Y_test = y_scaler.transform(Y_test)\n",
    "\n",
    "    scalers = {\"x\": x_scaler.state_dict() if cfg[\"standardize_features\"] else None,\n",
    "               \"y\": y_scaler.state_dict() if cfg[\"standardize_targets\"] else None}\n",
    "    return (X_train, Y_train, X_val, Y_val, X_test, Y_test,\n",
    "            train_idx, val_idx, test_idx, scalers)\n",
    "\n",
    "# =====================\n",
    "# Dataset & DataLoader\n",
    "# =====================\n",
    "class SprayDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, Y: np.ndarray, input_noise_std: float = 0.0, train: bool = False):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.Y = Y.astype(np.float32)\n",
    "        self.input_noise_std = float(input_noise_std)\n",
    "        self.train = bool(train)\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx].copy()\n",
    "        y = self.Y[idx].copy()\n",
    "        if self.train and self.input_noise_std > 0.0:\n",
    "            x = x + np.random.normal(0.0, self.input_noise_std, size=x.shape).astype(np.float32)\n",
    "        return torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "def make_dataloaders(Xtr, Ytr, Xv, Yv, Xte, Yte, cfg: dict):\n",
    "    ds_tr = SprayDataset(Xtr, Ytr, input_noise_std=cfg[\"input_noise_std\"], train=True)\n",
    "    ds_v  = SprayDataset(Xv, Yv, input_noise_std=0.0, train=False)\n",
    "    ds_te = SprayDataset(Xte, Yte, input_noise_std=0.0, train=False)\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=cfg[\"batch_size\"], shuffle=True, num_workers=cfg[\"num_workers\"], drop_last=False)\n",
    "    dl_v  = DataLoader(ds_v,  batch_size=cfg[\"batch_size\"], shuffle=False, num_workers=cfg[\"num_workers\"], drop_last=False)\n",
    "    dl_te = DataLoader(ds_te, batch_size=cfg[\"batch_size\"], shuffle=False, num_workers=cfg[\"num_workers\"], drop_last=False)\n",
    "    return dl_tr, dl_v, dl_te\n",
    "\n",
    "# =====================\n",
    "# Model\n",
    "# =====================\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, hidden: List[int], activation: str = \"relu\",\n",
    "                 dropout: float = 0.0, normalization: str = \"none\", weight_norm: bool = False,\n",
    "                 output_activation: str = \"none\"):\n",
    "        super().__init__()\n",
    "        acts = {\"relu\": nn.ReLU(), \"gelu\": nn.GELU(), \"prelu\": nn.PReLU(), \"tanh\": nn.Tanh()}\n",
    "        act = acts.get(activation, nn.ReLU())\n",
    "        layers = []\n",
    "        prev = in_dim\n",
    "        for h in hidden:\n",
    "            lin = nn.Linear(prev, h)\n",
    "            if weight_norm:\n",
    "                lin = nn.utils.weight_norm(lin)\n",
    "            layers.append(lin)\n",
    "            if normalization == \"batch\":\n",
    "                layers.append(nn.BatchNorm1d(h))\n",
    "            elif normalization == \"layer\":\n",
    "                layers.append(nn.LayerNorm(h))\n",
    "            layers.append(act)\n",
    "            if dropout and dropout > 0:\n",
    "                layers.append(nn.Dropout(p=dropout))\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        if output_activation == \"relu\":\n",
    "            self.out_act = nn.ReLU()\n",
    "        elif output_activation == \"softplus\":\n",
    "            self.out_act = nn.Softplus()\n",
    "        else:\n",
    "            self.out_act = nn.Identity()\n",
    "    def forward(self, x):\n",
    "        return self.out_act(self.net(x))\n",
    "\n",
    "def mixup_regression(x, y, alpha: float):\n",
    "    if alpha is None or alpha <= 0.0:\n",
    "        return x, y\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size, device=x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    mixed_y = lam * y + (1 - lam) * y[index, :]\n",
    "    return mixed_x, mixed_y\n",
    "\n",
    "def l1_regularization(model: nn.Module):\n",
    "    l1 = torch.tensor(0.0, device=next(model.parameters()).device)\n",
    "    for p in model.parameters():\n",
    "        l1 = l1 + p.abs().sum()\n",
    "    return l1\n",
    "\n",
    "def train_model(Xtr, Ytr, Xv, Yv, Xte, Yte, cfg: dict):\n",
    "    dl_tr, dl_v, dl_te = make_dataloaders(Xtr, Ytr, Xv, Yv, Xte, Yte, cfg)\n",
    "    model = MLP(in_dim=Xtr.shape[1], out_dim=Ytr.shape[1], hidden=cfg[\"hidden_sizes\"],\n",
    "                activation=cfg[\"activation\"], dropout=cfg[\"dropout\"],\n",
    "                normalization=cfg[\"normalization\"], weight_norm=cfg[\"weight_norm\"],\n",
    "                output_activation=cfg[\"output_activation\"]).to(device)\n",
    "\n",
    "    if cfg[\"optimizer\"].lower() == \"adamw\":\n",
    "        opt = AdamW(model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"weight_decay\"])\n",
    "    else:\n",
    "        opt = AdamW(model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"weight_decay\"])\n",
    "\n",
    "    if cfg[\"scheduler\"] == \"plateau\":\n",
    "        scheduler = ReduceLROnPlateau(opt, mode=\"min\", factor=cfg[\"plateau_factor\"],\n",
    "                                      patience=cfg[\"plateau_patience\"], verbose=True)\n",
    "    elif cfg[\"scheduler\"] == \"cosine\":\n",
    "        scheduler = CosineAnnealingLR(opt, T_max=cfg[\"cosine_T_max\"])\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=cfg[\"use_amp\"] and (device.type == \"cuda\"))\n",
    "    criterion = nn.MSELoss()\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"lr\": []}\n",
    "    best_val = float(\"inf\"); best_epoch = -1\n",
    "\n",
    "    for epoch in range(cfg[\"epochs\"]):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for xb, yb in dl_tr:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            xb, yb = mixup_regression(xb, yb, cfg[\"mixup_alpha\"])\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=cfg[\"use_amp\"] and (device.type == \"cuda\")):\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "                if cfg[\"l1_lambda\"] and cfg[\"l1_lambda\"] > 0.0:\n",
    "                    loss = loss + cfg[\"l1_lambda\"] * l1_regularization(model)\n",
    "            scaler.scale(loss).backward()\n",
    "            if cfg[\"gradient_clip_norm\"] is not None:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg[\"gradient_clip_norm\"])\n",
    "            scaler.step(opt); scaler.update()\n",
    "            train_loss += loss.item() * xb.size(0)\n",
    "        train_loss /= len(dl_tr.dataset)\n",
    "\n",
    "        model.eval(); val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in dl_v:\n",
    "                xb = xb.to(device); yb = yb.to(device)\n",
    "                preds = model(xb)\n",
    "                val_loss += criterion(preds, yb).item() * xb.size(0)\n",
    "        val_loss /= len(dl_v.dataset)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            if isinstance(scheduler, ReduceLROnPlateau):\n",
    "                scheduler.step(val_loss); current_lr = opt.param_groups[0][\"lr\"]\n",
    "            else:\n",
    "                scheduler.step(); current_lr = opt.param_groups[0][\"lr\"]\n",
    "        else:\n",
    "            current_lr = opt.param_groups[0][\"lr\"]\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"lr\"].append(current_lr)\n",
    "\n",
    "        improved = (best_val - val_loss) > cfg[\"min_delta\"]\n",
    "        if improved:\n",
    "            best_val = val_loss; best_epoch = epoch\n",
    "            torch.save(model.state_dict(), os.path.join(cfg[\"out_dir\"], \"best_model.pt\"))\n",
    "        if (epoch - best_epoch) >= cfg[\"early_stop_patience\"]:\n",
    "            print(f\"Early stopping at epoch {epoch}. Best val @ {best_epoch}: {best_val:.6f}\")\n",
    "            break\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1:03d}: train {train_loss:.6f} | val {val_loss:.6f} | lr {current_lr:.2e}\")\n",
    "\n",
    "    model.load_state_dict(torch.load(os.path.join(cfg[\"out_dir\"], \"best_model.pt\"), map_location=device))\n",
    "    return model, dl_tr, dl_v, dl_te, history\n",
    "\n",
    "def evaluate_dataloader(model, dataloader, y_scaler_state=None):\n",
    "    model.eval()\n",
    "    preds_list, targets_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dataloader:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            pred = model(xb)\n",
    "            preds_list.append(pred.cpu().numpy())\n",
    "            targets_list.append(yb.cpu().numpy())\n",
    "    preds = np.concatenate(preds_list, axis=0)\n",
    "    targets = np.concatenate(targets_list, axis=0)\n",
    "    if y_scaler_state is not None:\n",
    "        mu = np.array(y_scaler_state[\"mu\"]); sigma = np.array(y_scaler_state[\"sigma\"])\n",
    "        preds = preds * sigma + mu; targets = targets * sigma + mu\n",
    "    metrics = {\"MAE\": float(mean_absolute_error(targets, preds)),\n",
    "               \"RMSE\": float(np.sqrt(mean_squared_error(targets, preds))),\n",
    "               \"R2\": float(r2_score(targets, preds))}\n",
    "    return preds, targets, metrics\n",
    "\n",
    "def plot_history(history: dict, out_dir: str):\n",
    "    plt.figure(); plt.plot(history[\"train_loss\"], label=\"train_loss\"); plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"MSE Loss\"); plt.legend(); plt.grid(True); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"loss_curves.png\"), dpi=150); plt.show()\n",
    "    plt.figure(); plt.plot(history[\"lr\"], label=\"lr\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Learning Rate\"); plt.legend(); plt.grid(True); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"lr_curve.png\"), dpi=150); plt.show()\n",
    "\n",
    "# =====================\n",
    "# Main\n",
    "# =====================\n",
    "def main(cfg: dict):\n",
    "    set_seed(cfg[\"seed\"]); os.makedirs(cfg[\"out_dir\"], exist_ok=True)\n",
    "    df_raw = build_dataframe(cfg[\"root_dir\"], cfg[\"csv_glob\"])\n",
    "    if df_raw.empty:\n",
    "        raise SystemExit(f\"No data found under {cfg['root_dir']} with pattern {cfg['csv_glob']}\")\n",
    "    df = add_conditions_and_filter(df_raw, cfg)\n",
    "    (Xtr, Ytr, Xv, Yv, Xte, Yte, train_idx, val_idx, test_idx, scalers) = build_tensors_and_split(df, cfg)\n",
    "    with open(os.path.join(cfg[\"out_dir\"], \"scalers.json\"), \"w\") as f: json.dump(scalers, f)\n",
    "    model, dl_tr, dl_v, dl_te, history = train_model(Xtr, Ytr, Xv, Yv, Xte, Yte, cfg)\n",
    "    ysc = scalers[\"y\"] if cfg[\"standardize_targets\"] else None\n",
    "    preds_tr, t_tr, m_tr = evaluate_dataloader(model, dl_tr, y_scaler_state=ysc)\n",
    "    preds_v,  t_v,  m_v  = evaluate_dataloader(model, dl_v,  y_scaler_state=ysc)\n",
    "    preds_te, t_te, m_te = evaluate_dataloader(model, dl_te, y_scaler_state=ysc)\n",
    "    np.savez(os.path.join(cfg[\"out_dir\"], \"predictions.npz\"),\n",
    "             train_preds=preds_tr, train_targets=t_tr,\n",
    "             val_preds=preds_v, val_targets=t_v,\n",
    "             test_preds=preds_te, test_targets=t_te)\n",
    "    metrics = {\"train\": m_tr, \"val\": m_v, \"test\": m_te}\n",
    "    with open(os.path.join(cfg[\"out_dir\"], \"metrics.json\"), \"w\") as f: json.dump(metrics, f, indent=2)\n",
    "    plot_history(history, cfg[\"out_dir\"])\n",
    "    print(\"Metrics:\"); print(json.dumps(metrics, indent=2))\n",
    "\n",
    "# To run:\n",
    "# main(CONFIG)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
